{
  "cells": [
    {
      "metadata": {
        "_uuid": "c3240a53557cb9011ce9139567c716c84de5900f"
      },
      "cell_type": "markdown",
      "source": "**Predicting Sales Prices**\n\nAbout the Housing Prices: Advance Regression Data:\n1.  It is already split into sets - training and test data\n\n**Importing the recommended packages for Python**"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# This first set of packages include Pandas, for data manipulation, numpy for mathematical computation and matplotlib & seaborn, for visualisation.\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep')\nprint('Data Manipulation, Mathematical Computation and Visualisation packages imported!')\n\n# Stats Package\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats.stats import pearsonr\nprint('Stats Package imported!')\n\n# Metrics used for measuring the accuracy and performance of the models\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nprint('Metrics packages imported!')\n\n# Algorithms used for modeling\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nimport xgboost as xgb\nprint('Algorithm packages imported!')\n\n# Pipeline and scaling preprocessing will be used for models that are sensitive\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nprint('Pipeline and Preprocessing Packages imported!')\n\n# Model selection packages used for sampling dataset and optimising parameters\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import ShuffleSplit\nprint('Model Packages imported!')\n\n# Set visualisation colours\nmycols = [\"#66c2ff\", \"#5cd6d6\", \"#00cc99\", \"#85e085\", \"#ffd966\", \"#ffb366\", \"#ffb3b3\", \"#dab3ff\", \"#c2c2d6\"]\nsns.set_palette(palette = mycols, n_colors = 4)\nprint('My colours are ready! :)')\n\n# To ignore annoying warning\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nprint('Deprecation warning will be ignored!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "de335deda7628729d0567e2cdf0e569b54cad9f4"
      },
      "cell_type": "markdown",
      "source": "Data Exploration"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint('Training and test data have been imported!')\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Drop Id from train and test data\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\nprint(\"\\nTraining shape: {}\".format(train.shape))\nprint(\"Test shape: {}\".format(test.shape))\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "457d58e9d720c5df2940703cb6c07bed22113e17"
      },
      "cell_type": "markdown",
      "source": "As we can tell from the above:\n* The **training data** has 1460 observations\n* The **test data** has 1459 observations\n\n**Lets find out more about the different columns in the data:**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a56db25c341e7af062d9a60f512ea61b0fbe8a48"
      },
      "cell_type": "code",
      "source": "# Count the column types\ntrain.dtypes.value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2c29ea55354c7c872c78660cad35e76d0af4c31e"
      },
      "cell_type": "markdown",
      "source": "There are two data types I noticed from looking at the different columns:\n1. Numerical\n2. Categorical\n\nThat being said, I will tackle this problem in the data manipulation portion. Where I plan to use **'Hot Encoding'** to change the data from numerical to categorical\n\nSince we are predicting the 'SalePrice', lets figure out more about it"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a19f21f4d17c4faa28c5e9f4f4b2ca531c5e4317"
      },
      "cell_type": "code",
      "source": "train['SalePrice'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4408e0b3d5a354af89615f71561484114cf84c4"
      },
      "cell_type": "code",
      "source": "#histogram of SalePrice\nsns.distplot(train['SalePrice']);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5d69c1506009f8a73d4344c11a88852ec9fa822c"
      },
      "cell_type": "markdown",
      "source": "From the above, what we could tell from the SalePrice is that the data is not normally distributed, positvely skewed.\n\nNow that we have that done, lets do a comparison!\nI want to find out what is the most correlated to 'SalePrice'"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01970cb1dc2ca93291c455427855b71abe7f641b"
      },
      "cell_type": "code",
      "source": "#saleprice correlation matrix\ncorrelation = train.corr()\nk = 10 #number of variables for heatmap\ncorr = correlation.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[corr].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cmap=\"Greens\", cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=corr.values, xticklabels=corr.values)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d3ec0ee1d033fca8e9e58ada9fca3ddbaeb7bd0a"
      },
      "cell_type": "markdown",
      "source": "What we could tell from the correlation above is that top 10 variables that have the highest correlation with 'SalePrice' is:\n1. OverallQual\n2. GrLivArea\n3. GarageCars\n4. GarageArea\n5. otalBsmtSF\n6. 1stFlrSF\n7. FullBath\n8. TotRmsAbvGrd\n9. YearBuilt\n\nWith that being said, lets look deeper into 'OverallQual', 'GrLivArea', 'GarageCars' and Its Correlation to 'SalePrice' to make sure this true."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0dbcf6440ed2c26a5db9d4f45b0bfd7f7c9e8e6e"
      },
      "cell_type": "code",
      "source": "#OverallQual\nvar = 'OverallQual'\nvis = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=vis)\nfig.axis(ymin=0, ymax=800000);\n\n#GrLivArea\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n\n\n#GarageCars\nvar = 'GarageCars'\nvis = pd.concat([train['SalePrice'], train[var]], axis=1)\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=vis)\nfig.axis(ymin=0, ymax=800000);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "abca6d6b7b7fab8a23f0505956ba0d691aaf8eb0"
      },
      "cell_type": "markdown",
      "source": "When looking at all of the three graphs above, we notice they all have a positive correlation with 'SalePrice'.\n\nAlthough I did notice in 'GrLivArea' there are a couple of outliers that we do have to address in the bottom and top right corner, we will look into this later in the data manipulation.\nWe can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers.\nTherefore, we can safely drop them."
    },
    {
      "metadata": {
        "_uuid": "220f704a327bfa0585445a43cbd6ec59bcfd4d51"
      },
      "cell_type": "markdown",
      "source": "**Feature Selection**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e32e087f85581c9e171ea71c13ac716ef2ba670"
      },
      "cell_type": "code",
      "source": "plt.subplots(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\ng = sns.regplot(x=train['GrLivArea'], y=train['SalePrice'], fit_reg=False).set_title(\"Before\")\n\n# Delete outliers\nplt.subplot(1, 2, 2)                                                                                \ntrain = train.drop(train[(train['GrLivArea']>4000)].index)\ng = sns.regplot(x=train['GrLivArea'], y=train['SalePrice'], fit_reg=False).set_title(\"After\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "984f77e1b93083527fc8be4195e83a8fb63261b5"
      },
      "cell_type": "code",
      "source": "#Save the length of train and test data so we could split later when we split later when we do modelling at the end!\nntrain = train.shape[0]\nntest = test.shape[0]\n\n# Also save the target value, as we will remove this\ntargetval = train.SalePrice.values\n\n# concatenate training and test data into data\ndata = pd.concat((train, test)).reset_index(drop=True)\ndata.drop(['SalePrice'], axis=1, inplace=True)\n\nprint(\"Concated Data Shape: {}\".format(data.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d588f75cfdd12417f2ac7671c77c7a6fa5897db2"
      },
      "cell_type": "markdown",
      "source": "**Missing Data**"
    },
    {
      "metadata": {
        "_uuid": "ee8301283cf10aba2e5b4f63bb1130037faec3b5"
      },
      "cell_type": "markdown",
      "source": "Yes, there are a lot of NULL values, this does not mean that we should remove them. Instead I think we should impute them, this meaning that if the dataset contain any missing values, often encoded as blanks, we should put a placeholder in its place like 'NONE'.\n\nIn the below code, I am changing the categorical data to nominal so that way it will be easier to compute later.\nThe reason why I suggest this is because if we remove values that are blank we may affect most of the data, for instance the pool data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b11ac752d0faa57ec11a43bb930eb487701dc8c6"
      },
      "cell_type": "code",
      "source": "# Using data description, fill these missing values with \"None\"\nfor col in (\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\",\n           \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n           \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\n            \"BsmtFinType2\", \"MSSubClass\", \"MasVnrType\"):\n    data[col] = data[col].fillna(\"None\")\nprint(\"'None' - treated...\")\n\n# The area of the lot out front is likely to be similar to the houses in the local neighbourhood\n# Therefore, let's use the median value of the houses in the neighbourhood to fill this feature\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nprint(\"'LotFrontage' - treated...\")\n\n# Using data description, fill these missing values with 0 \nfor col in (\"GarageYrBlt\", \"GarageArea\", \"GarageCars\", \"BsmtFinSF1\", \n           \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\",\n           \"BsmtFullBath\", \"BsmtHalfBath\"):\n    data[col] = data[col].fillna(0)\nprint(\"'0' - treated...\")\n\n\n# Fill these features with their mode, the most commonly occuring value. This is okay since there are a low number of missing values for these features\ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\ndata['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])\ndata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])\ndata[\"Functional\"] = data[\"Functional\"].fillna(data['Functional'].mode()[0])\nprint(\"'mode' - treated...\")\n\ndata_na = data.isnull().sum()\nprint(\"Features with missing values: \", data_na.drop(data_na[data_na == 0].index))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d09e0cbd87f9a6884ba1f61a1ece59bf279a29bc"
      },
      "cell_type": "code",
      "source": "# From inspection, we can remove Utilities\ndata = data.drop(['Utilities'], axis=1)\n\ndata_na = data.isnull().sum()\nprint(\"Features with missing values: \", len(data_na.drop(data_na[data_na == 0].index)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "418f2b21eb4a76618cb95b42f2ea50d3fe124be2"
      },
      "cell_type": "markdown",
      "source": "Feature Engineering"
    },
    {
      "metadata": {
        "_uuid": "c8a237ca7a2ee9057da957de372d82a9f60cf711"
      },
      "cell_type": "markdown",
      "source": "Polynomials"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bba98200e1858ee23b641b053548a530ac0f751a"
      },
      "cell_type": "code",
      "source": "#All the features that are in metrics:\n\n# Quadratic\ndata[\"OverallQual-2\"] = data[\"OverallQual\"] ** 2\ndata[\"GrLivArea-2\"] = data[\"GrLivArea\"] ** 2\ndata[\"GarageCars-2\"] = data[\"GarageCars\"] ** 2\ndata[\"GarageArea-2\"] = data[\"GarageArea\"] ** 2\ndata[\"TotalBsmtSF-2\"] = data[\"TotalBsmtSF\"] ** 2\ndata[\"1stFlrSF-2\"] = data[\"1stFlrSF\"] ** 2\ndata[\"FullBath-2\"] = data[\"FullBath\"] ** 2\ndata[\"TotRmsAbvGrd-2\"] = data[\"TotRmsAbvGrd\"] ** 2\ndata[\"Fireplaces-2\"] = data[\"Fireplaces\"] ** 2\ndata[\"MasVnrArea-2\"] = data[\"MasVnrArea\"] ** 2\ndata[\"BsmtFinSF1-2\"] = data[\"BsmtFinSF1\"] ** 2\ndata[\"LotFrontage-2\"] = data[\"LotFrontage\"] ** 2\ndata[\"WoodDeckSF-2\"] = data[\"WoodDeckSF\"] ** 2\ndata[\"OpenPorchSF-2\"] = data[\"OpenPorchSF\"] ** 2\ndata[\"2ndFlrSF-2\"] = data[\"2ndFlrSF\"] ** 2\nprint(\"Quadratics done!...\")\n\n# Cubic\ndata[\"OverallQual-3\"] = data[\"OverallQual\"] ** 3\ndata[\"GrLivArea-3\"] = data[\"GrLivArea\"] ** 3\ndata[\"GarageCars-3\"] = data[\"GarageCars\"] ** 3\ndata[\"GarageArea-3\"] = data[\"GarageArea\"] ** 3\ndata[\"TotalBsmtSF-3\"] = data[\"TotalBsmtSF\"] ** 3\ndata[\"1stFlrSF-3\"] = data[\"1stFlrSF\"] ** 3\ndata[\"FullBath-3\"] = data[\"FullBath\"] ** 3\ndata[\"TotRmsAbvGrd-3\"] = data[\"TotRmsAbvGrd\"] ** 3\ndata[\"Fireplaces-3\"] = data[\"Fireplaces\"] ** 3\ndata[\"MasVnrArea-3\"] = data[\"MasVnrArea\"] ** 3\ndata[\"BsmtFinSF1-3\"] = data[\"BsmtFinSF1\"] ** 3\ndata[\"LotFrontage-3\"] = data[\"LotFrontage\"] ** 3\ndata[\"WoodDeckSF-3\"] = data[\"WoodDeckSF\"] ** 3\ndata[\"OpenPorchSF-3\"] = data[\"OpenPorchSF\"] ** 3\ndata[\"2ndFlrSF-3\"] = data[\"2ndFlrSF\"] ** 3\nprint(\"Cubics done!...\")\n\n# Square Root\ndata[\"OverallQual-Sq\"] = np.sqrt(data[\"OverallQual\"])\ndata[\"GrLivArea-Sq\"] = np.sqrt(data[\"GrLivArea\"])\ndata[\"GarageCars-Sq\"] = np.sqrt(data[\"GarageCars\"])\ndata[\"GarageArea-Sq\"] = np.sqrt(data[\"GarageArea\"])\ndata[\"TotalBsmtSF-Sq\"] = np.sqrt(data[\"TotalBsmtSF\"])\ndata[\"1stFlrSF-Sq\"] = np.sqrt(data[\"1stFlrSF\"])\ndata[\"FullBath-Sq\"] = np.sqrt(data[\"FullBath\"])\ndata[\"TotRmsAbvGrd-Sq\"] = np.sqrt(data[\"TotRmsAbvGrd\"])\ndata[\"Fireplaces-Sq\"] = np.sqrt(data[\"Fireplaces\"])\ndata[\"MasVnrArea-Sq\"] = np.sqrt(data[\"MasVnrArea\"])\ndata[\"BsmtFinSF1-Sq\"] = np.sqrt(data[\"BsmtFinSF1\"])\ndata[\"LotFrontage-Sq\"] = np.sqrt(data[\"LotFrontage\"])\ndata[\"WoodDeckSF-Sq\"] = np.sqrt(data[\"WoodDeckSF\"])\ndata[\"OpenPorchSF-Sq\"] = np.sqrt(data[\"OpenPorchSF\"])\ndata[\"2ndFlrSF-Sq\"] = np.sqrt(data[\"2ndFlrSF\"])\nprint(\"Roots done!...\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "26c40ffb348fe9bf10a131a4376426d742afadd3"
      },
      "cell_type": "markdown",
      "source": "Changing the data from Categorical to Nominal"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5d0b62da4eb905056ed08c5df147b8eab23f982"
      },
      "cell_type": "code",
      "source": "#Basement Quality\ndata['BsmtQual'] = data['BsmtQual'].map({\"None\":0, \"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})\ndata['BsmtQual'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c528ed745e78c683a3ba2d3f4d79b012ebab509"
      },
      "cell_type": "code",
      "source": "#Basement Condition\ndata['BsmtCond'] = data['BsmtCond'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})\ndata['BsmtCond'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6d5982eae3a4ec0468b97a43237dc08cfed3ec3"
      },
      "cell_type": "code",
      "source": "#Basement Exposure\ndata['BsmtExposure'] = data['BsmtExposure'].map({\"None\":0, \"No\":1, \"Mn\":2, \"Av\":3, \"Gd\":4})\ndata['BsmtExposure'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc211121d6e8551e2b401bd6e9a6398c00c72fcb"
      },
      "cell_type": "code",
      "source": "#BsmtFinType\ndata = pd.get_dummies(data, columns = [\"BsmtFinType1\"], prefix=\"BsmtFinType1\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4ccc406868b95e16e5a62bf1aab2601e36c3f08"
      },
      "cell_type": "code",
      "source": "#BsmtFinSF1_Band\ndata['BsmtFinSF1_Band'] = pd.cut(data['BsmtFinSF1'], 4)\ndata['BsmtFinSF1_Band'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95c3dc5379fc72ccd347dedbc1e0046ccd56af44"
      },
      "cell_type": "code",
      "source": "#BsmtFinSF1\ndata.loc[data['BsmtFinSF1']<=1002.5, 'BsmtFinSF1'] = 1\ndata.loc[(data['BsmtFinSF1']>1002.5) & (data['BsmtFinSF1']<=2005), 'BsmtFinSF1'] = 2\ndata.loc[(data['BsmtFinSF1']>2005) & (data['BsmtFinSF1']<=3007.5), 'BsmtFinSF1'] = 3\ndata.loc[data['BsmtFinSF1']>3007.5, 'BsmtFinSF1'] = 4\ndata['BsmtFinSF1'] = data['BsmtFinSF1'].astype(int)\ndata.drop('BsmtFinSF1_Band', axis=1, inplace=True)\ndata = pd.get_dummies(data, columns = [\"BsmtFinSF1\"], prefix=\"BsmtFinSF1\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e1d749fddd3856a558c9f92947b8afadf1b3be6"
      },
      "cell_type": "code",
      "source": "#BsmtFinType2\ndata = pd.get_dummies(data, columns = [\"BsmtFinType2\"], prefix=\"BsmtFinType2\")\ndata['BsmtFinSf2_Flag'] = data['BsmtFinSF2'].map(lambda x:0 if x==0 else 1)\ndata.drop('BsmtFinSF2', axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb2d7504c9dce1d08122da2baf3df858dc8d9108"
      },
      "cell_type": "code",
      "source": "#BsmtUnfSF_Band\ndata['BsmtUnfSF_Band'] = pd.cut(data['BsmtUnfSF'], 3)\ndata['BsmtUnfSF_Band'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c1859de9e588136691965969dbee785b0ce6ee9f"
      },
      "cell_type": "code",
      "source": "data.loc[data['BsmtUnfSF']<=778.667, 'BsmtUnfSF'] = 1\ndata.loc[(data['BsmtUnfSF']>778.667) & (data['BsmtUnfSF']<=1557.333), 'BsmtUnfSF'] = 2\ndata.loc[data['BsmtUnfSF']>1557.333, 'BsmtUnfSF'] = 3\ndata['BsmtUnfSF'] = data['BsmtUnfSF'].astype(int)\n\ndata.drop('BsmtUnfSF_Band', axis=1, inplace=True)\n\ndata = pd.get_dummies(data, columns = [\"BsmtUnfSF\"], prefix=\"BsmtUnfSF\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c033f8833e416515e8878391c313a57f9780e242"
      },
      "cell_type": "code",
      "source": "#totalBsmtSF_Band\ndata['TotalBsmtSF_Band'] = pd.cut(data['TotalBsmtSF'], 10)\ndata['TotalBsmtSF_Band'].unique()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e323cc943072beab0362a2b6c0ce0b2ffd41064a"
      },
      "cell_type": "code",
      "source": "data.loc[data['TotalBsmtSF']<=509.5, 'TotalBsmtSF'] = 1\ndata.loc[(data['TotalBsmtSF']>509.5) & (data['TotalBsmtSF']<=1019), 'TotalBsmtSF'] = 2\ndata.loc[(data['TotalBsmtSF']>1019) & (data['TotalBsmtSF']<=1528.5), 'TotalBsmtSF'] = 3\ndata.loc[(data['TotalBsmtSF']>1528.5) & (data['TotalBsmtSF']<=2038), 'TotalBsmtSF'] = 4\ndata.loc[(data['TotalBsmtSF']>2038) & (data['TotalBsmtSF']<=2547.5), 'TotalBsmtSF'] = 5\ndata.loc[(data['TotalBsmtSF']>2547.5) & (data['TotalBsmtSF']<=3057), 'TotalBsmtSF'] = 6\ndata.loc[(data['TotalBsmtSF']>3057) & (data['TotalBsmtSF']<=3566.5), 'TotalBsmtSF'] = 7\ndata.loc[data['TotalBsmtSF']>3566.5, 'TotalBsmtSF'] = 8\ndata['TotalBsmtSF'] = data['TotalBsmtSF'].astype(int)\n\ndata.drop('TotalBsmtSF_Band', axis=1, inplace=True)\n\ndata = pd.get_dummies(data, columns = [\"TotalBsmtSF\"], prefix=\"TotalBsmtSF\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30800b3f9f95ca09029cd7013bd11698e5470e51"
      },
      "cell_type": "code",
      "source": "#1stFlrSF_Band'\ndata['1stFlrSF_Band'] = pd.cut(data['1stFlrSF'], 6)\ndata['1stFlrSF_Band'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "97bd3ed35c4f1e98076fb40c4fe3329fce865994"
      },
      "cell_type": "code",
      "source": "data.loc[data['1stFlrSF']<=1127.5, '1stFlrSF'] = 1\ndata.loc[(data['1stFlrSF']>1127.5) & (data['1stFlrSF']<=1921), '1stFlrSF'] = 2\ndata.loc[(data['1stFlrSF']>1921) & (data['1stFlrSF']<=2714.5), '1stFlrSF'] = 3\ndata.loc[(data['1stFlrSF']>2714.5) & (data['1stFlrSF']<=3508), '1stFlrSF'] = 4\ndata.loc[(data['1stFlrSF']>3508) & (data['1stFlrSF']<=4301.5), '1stFlrSF'] = 5\ndata.loc[data['1stFlrSF']>4301.5, '1stFlrSF'] = 6\ndata['1stFlrSF'] = data['1stFlrSF'].astype(int)\n\ndata.drop('1stFlrSF_Band', axis=1, inplace=True)\n\ndata = pd.get_dummies(data, columns = [\"1stFlrSF\"], prefix=\"1stFlrSF\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "624b2430486b3761a0e72eaeebd61e71fbd6c6ef"
      },
      "cell_type": "code",
      "source": "#2ndFlrSF_Band'\ndata['2ndFlrSF_Band'] = pd.cut(data['2ndFlrSF'], 6)\ndata['2ndFlrSF_Band'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d498ea4b7c69123c600b5262c6fa683828771370"
      },
      "cell_type": "code",
      "source": "data.loc[data['2ndFlrSF']<=310.333, '2ndFlrSF'] = 1\ndata.loc[(data['2ndFlrSF']>310.333) & (data['2ndFlrSF']<=620.667), '2ndFlrSF'] = 2\ndata.loc[(data['2ndFlrSF']>620.667) & (data['2ndFlrSF']<=931), '2ndFlrSF'] = 3\ndata.loc[(data['2ndFlrSF']>931) & (data['2ndFlrSF']<=1241.333), '2ndFlrSF'] = 4\ndata.loc[(data['2ndFlrSF']>1241.333) & (data['2ndFlrSF']<=1551.667), '2ndFlrSF'] = 5\ndata.loc[data['2ndFlrSF']>1551.667, '2ndFlrSF'] = 6\ndata['2ndFlrSF'] = data['2ndFlrSF'].astype(int)\n\ndata.drop('2ndFlrSF_Band', axis=1, inplace=True)\n\ndata = pd.get_dummies(data, columns = [\"2ndFlrSF\"], prefix=\"2ndFlrSF\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c226a56d6cacba740e36ceac022a9a83e8e6925"
      },
      "cell_type": "code",
      "source": "#LowQualFinSF\ndata['LowQualFinSF_Flag'] = data['LowQualFinSF'].map(lambda x:0 if x==0 else 1)\ndata.drop('LowQualFinSF', axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9869e6587e19ca053da92115680bd77de40a14ee"
      },
      "cell_type": "code",
      "source": "#Number of baths\ndata['TotalBathrooms'] = data['BsmtHalfBath'] + data['BsmtFullBath'] + data['HalfBath'] + data['FullBath']\n\ncolumns = ['BsmtHalfBath', 'BsmtFullBath', 'HalfBath', 'FullBath']\ndata.drop(columns, axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef309a5fce0159a76b83b4e3481b8a54fefd2b56"
      },
      "cell_type": "code",
      "source": "#Kitchen Quality\ndata['KitchenQual'] = data['KitchenQual'].map({\"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})\ndata['KitchenQual'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f46926ed5cf56f37a2142ca30cf3b39178ff215c"
      },
      "cell_type": "code",
      "source": "#Fire Place Quality\ndata['FireplaceQu'] = data['FireplaceQu'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})\ndata['FireplaceQu'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab72ea80e13b831c2c431d278476f8d4c43e54b9"
      },
      "cell_type": "code",
      "source": "#GrivArea\ndata['GrLivArea_Band'] = pd.cut(data['GrLivArea'], 6)\ndata['GrLivArea_Band'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ece3fa08623f120ae6c66cee001cb626371f70f"
      },
      "cell_type": "code",
      "source": "data.loc[data['GrLivArea']<=1127.5, 'GrLivArea'] = 1\ndata.loc[(data['GrLivArea']>1127.5) & (data['GrLivArea']<=1921), 'GrLivArea'] = 2\ndata.loc[(data['GrLivArea']>1921) & (data['GrLivArea']<=2714.5), 'GrLivArea'] = 3\ndata.loc[(data['GrLivArea']>2714.5) & (data['GrLivArea']<=3508), 'GrLivArea'] = 4\ndata.loc[(data['GrLivArea']>3508) & (data['GrLivArea']<=4301.5), 'GrLivArea'] = 5\ndata.loc[data['GrLivArea']>4301.5, 'GrLivArea'] = 6\ndata['GrLivArea'] = data['GrLivArea'].astype(int)\n\ndata.drop('GrLivArea_Band', axis=1, inplace=True)\ndata = pd.get_dummies(data, columns = [\"GrLivArea\"], prefix=\"GrLivArea\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd0e3dc6027928fd3ecba627a6b8208931d80e7a"
      },
      "cell_type": "code",
      "source": "#MSSubClass\ndata['MSSubClass'] = data['MSSubClass'].astype(str)\ndata = pd.get_dummies(data, columns = [\"MSSubClass\"], prefix=\"MSSubClass\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53b90ef6225a31f394e71930ad392078e3642d04"
      },
      "cell_type": "code",
      "source": "#BldgType\ndata['BldgType'] = data['BldgType'].astype(str)\ndata = pd.get_dummies(data, columns = [\"BldgType\"], prefix=\"BldgType\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de7ff4af0f6adaf637cac5ca3e5fe617f66d7e91"
      },
      "cell_type": "code",
      "source": "#HouseStyle\ndata['HouseStyle'] = data['HouseStyle'].map({\"2Story\":\"2Story\", \"1Story\":\"1Story\", \"1.5Fin\":\"1.5Story\", \"1.5Unf\":\"1.5Story\", \n                                                     \"SFoyer\":\"SFoyer\", \"SLvl\":\"SLvl\", \"2.5Unf\":\"2.5Story\", \"2.5Fin\":\"2.5Story\"})\ndata = pd.get_dummies(data, columns = [\"HouseStyle\"], prefix=\"HouseStyle\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e355c138061390693acb5a7a7469aa3bbde239d9"
      },
      "cell_type": "code",
      "source": "#Remodelling to Categorical\ntrain['Remod_Diff'] = train['YearRemodAdd'] - train['YearBuilt']\ndata['Remod_Diff'] = data['YearRemodAdd'] - data['YearBuilt']\ndata.drop('YearRemodAdd', axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80c381343e994c7fa061899daa6a7ee5b6d3c006"
      },
      "cell_type": "code",
      "source": "#Year Built\ndata['YearBuilt_Band'] = pd.cut(data['YearBuilt'], 7)\ndata['YearBuilt_Band'].unique()\ndata['YearBuilt_Band'] = pd.cut(data['YearBuilt'], 7)\ndata['YearBuilt_Band'].unique()\ndata.loc[data['YearBuilt']<=1892, 'YearBuilt'] = 1\ndata.loc[(data['YearBuilt']>1892) & (data['YearBuilt']<=1911), 'YearBuilt'] = 2\ndata.loc[(data['YearBuilt']>1911) & (data['YearBuilt']<=1931), 'YearBuilt'] = 3\ndata.loc[(data['YearBuilt']>1931) & (data['YearBuilt']<=1951), 'YearBuilt'] = 4\ndata.loc[(data['YearBuilt']>1951) & (data['YearBuilt']<=1971), 'YearBuilt'] = 5\ndata.loc[(data['YearBuilt']>1971) & (data['YearBuilt']<=1990), 'YearBuilt'] = 6\ndata.loc[data['YearBuilt']>1990, 'YearBuilt'] = 7\ndata['YearBuilt'] = data['YearBuilt'].astype(int)\n\ndata.drop('YearBuilt_Band', axis=1, inplace=True)\ndata = pd.get_dummies(data, columns = [\"YearBuilt\"], prefix=\"YearBuilt\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d429fe39fcd48757311cbfd3ccbb3246885a35f"
      },
      "cell_type": "code",
      "source": "#Foundation\ndata = pd.get_dummies(data, columns = [\"Foundation\"], prefix=\"Foundation\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fd6237ce6318b493b2febc29de5b948516e0eea1"
      },
      "cell_type": "code",
      "source": "#Functional\ndata['Functional'] = data['Functional'].map({\"Sev\":1, \"Maj2\":2, \"Maj1\":3, \"Mod\":4, \"Min2\":5, \"Min1\":6, \"Typ\":7})\ndata['Functional'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5578aa051f40ff3d6cae1d124851854378f80f7"
      },
      "cell_type": "code",
      "source": "#Roofstyle\ndata = pd.get_dummies(data, columns = [\"RoofStyle\"], prefix=\"RoofStyle\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58c98b1591f33f4b31c193c8d267e5c68e668ca8"
      },
      "cell_type": "code",
      "source": "#Roof Material\ndata = pd.get_dummies(data, columns = [\"RoofMatl\"], prefix=\"RoofMatl\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c4dbdd74c2a1b9436780d7b516b00fec1731a4c"
      },
      "cell_type": "code",
      "source": "#Exterior1st and 2nd floor\ndef Exter2(col):\n    if col['Exterior2nd'] == col['Exterior1st']:\n        return 1\n    else:\n        return 0\n    \ndata['ExteriorMatch_Flag'] = data.apply(Exter2, axis=1)\ndata.drop('Exterior2nd', axis=1, inplace=True)\n\ndata = pd.get_dummies(data, columns = [\"Exterior1st\"], prefix=\"Exterior1st\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01fa63aeae471ca87db040e8a61856cc1b84d952"
      },
      "cell_type": "code",
      "source": "#Masonry veneer type\ndata = pd.get_dummies(data, columns = [\"MasVnrType\"], prefix=\"MasVnrType\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af6e2e6b2df1418b532228f581521e1a983bf0d4"
      },
      "cell_type": "code",
      "source": "#MasVnrArea - No correlation to the SalePrice\ndata.drop('MasVnrArea', axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "778d2050609162139db0b490fdac888d513a83a9"
      },
      "cell_type": "code",
      "source": "#External Quality\ndata['ExterQual'] = data['ExterQual'].map({\"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})\ndata['ExterQual'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3096a593de4eb44970c7c0c8646f3bc51c775dbc"
      },
      "cell_type": "code",
      "source": "#External Condition\ndata = pd.get_dummies(data, columns = [\"ExterCond\"], prefix=\"ExterCond\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0033ce1a264cc70d2dabb0bcc780f17ef9c4717c"
      },
      "cell_type": "code",
      "source": "#GarageType\ndata = pd.get_dummies(data, columns = [\"GarageType\"], prefix=\"GarageType\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19df380e8d6cd1a26665c97d41b559b5dcc8b3b7"
      },
      "cell_type": "code",
      "source": "#Year garage was built\ndata['GarageYrBlt_Band'] = pd.qcut(data['GarageYrBlt'], 3)\ndata['GarageYrBlt_Band'].unique()\n\ndata.loc[data['GarageYrBlt']<=1964, 'GarageYrBlt'] = 1\ndata.loc[(data['GarageYrBlt']>1964) & (data['GarageYrBlt']<=1996), 'GarageYrBlt'] = 2\ndata.loc[data['GarageYrBlt']>1996, 'GarageYrBlt'] = 3\ndata['GarageYrBlt'] = data['GarageYrBlt'].astype(int)\n\ndata.drop('GarageYrBlt_Band', axis=1, inplace=True)\ndata = pd.get_dummies(data, columns = [\"GarageYrBlt\"], prefix=\"GarageYrBlt\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "839e31114b791b45f81e58c5849c06650af970c5"
      },
      "cell_type": "code",
      "source": "#Garage Finish\ndata = pd.get_dummies(data, columns = [\"GarageFinish\"], prefix=\"GarageFinish\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe1922ef6fb169bf270cfc5b95bce3c146db0033"
      },
      "cell_type": "code",
      "source": "#GarageArea_Band\ndata['GarageArea_Band'] = pd.cut(data['GarageArea'], 3)\ndata['GarageArea_Band'].unique()\n\ndata.loc[data['GarageArea']<=496, 'GarageArea'] = 1\ndata.loc[(data['GarageArea']>496) & (data['GarageArea']<=992), 'GarageArea'] = 2\ndata.loc[data['GarageArea']>992, 'GarageArea'] = 3\ndata['GarageArea'] = data['GarageArea'].astype(int)\n\ndata.drop('GarageArea_Band', axis=1, inplace=True)\n\ndata = pd.get_dummies(data, columns = [\"GarageArea\"], prefix=\"GarageArea\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7f134a42e1c578f695ed51d29c1a5dcf4582b015"
      },
      "cell_type": "code",
      "source": "#Garage Quality\ndata['GarageQual'] = data['GarageQual'].map({\"None\":\"None\", \"Po\":\"Low\", \"Fa\":\"Low\", \"TA\":\"TA\", \"Gd\":\"High\", \"Ex\":\"High\"})\ndata['GarageQual'].unique()\ndata = pd.get_dummies(data, columns = [\"GarageQual\"], prefix=\"GarageQual\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c98925c9350d67fc6e5481e215487fc37ffb498"
      },
      "cell_type": "code",
      "source": "#Garage Condition\ndata['GarageCond'] = data['GarageCond'].map({\"None\":\"None\", \"Po\":\"Low\", \"Fa\":\"Low\", \"TA\":\"TA\", \"Gd\":\"High\", \"Ex\":\"High\"})\ndata['GarageCond'].unique()\ndata = pd.get_dummies(data, columns = [\"GarageCond\"], prefix=\"GarageCond\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28e023c1ed1149d2da3016b78d0d6a00442ba2eb"
      },
      "cell_type": "code",
      "source": "#Wooddeck\ndef WoodDeckFlag(col):\n    if col['WoodDeckSF'] == 0:\n        return 1\n    else:\n        return 0\n    \ndata['NoWoodDeck_Flag'] = data.apply(WoodDeckFlag, axis=1)\n\ndata['WoodDeckSF_Band'] = pd.cut(data['WoodDeckSF'], 4)\n\ndata.loc[data['WoodDeckSF']<=356, 'WoodDeckSF'] = 1\ndata.loc[(data['WoodDeckSF']>356) & (data['WoodDeckSF']<=712), 'WoodDeckSF'] = 2\ndata.loc[(data['WoodDeckSF']>712) & (data['WoodDeckSF']<=1068), 'WoodDeckSF'] = 3\ndata.loc[data['WoodDeckSF']>1068, 'WoodDeckSF'] = 4\ndata['WoodDeckSF'] = data['WoodDeckSF'].astype(int)\ndata.drop('WoodDeckSF_Band', axis=1, inplace=True)\n\ndata = pd.get_dummies(data, columns = [\"WoodDeckSF\"], prefix=\"WoodDeckSF\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cd15adaba4102625f16cce9b89cc2134790cd82"
      },
      "cell_type": "code",
      "source": "#Total Surface Area of Porch\n\ndata['TotalPorchSF'] = data['OpenPorchSF'] + data['OpenPorchSF'] + data['EnclosedPorch'] + data['3SsnPorch'] + data['ScreenPorch'] \ntrain['TotalPorchSF'] = train['OpenPorchSF'] + train['OpenPorchSF'] + train['EnclosedPorch'] + train['3SsnPorch'] + train['ScreenPorch']\n\ndef PorchFlag(col):\n    if col['TotalPorchSF'] == 0:\n        return 1\n    else:\n        return 0\n    \ndata['NoPorch_Flag'] = data.apply(PorchFlag, axis=1)\n\ndata['TotalPorchSF_Band'] = pd.cut(data['TotalPorchSF'], 4)\ndata['TotalPorchSF_Band'].unique()\n\ndata.loc[data['TotalPorchSF']<=431, 'TotalPorchSF'] = 1\ndata.loc[(data['TotalPorchSF']>431) & (data['TotalPorchSF']<=862), 'TotalPorchSF'] = 2\ndata.loc[(data['TotalPorchSF']>862) & (data['TotalPorchSF']<=1293), 'TotalPorchSF'] = 3\ndata.loc[data['TotalPorchSF']>1293, 'TotalPorchSF'] = 4\ndata['TotalPorchSF'] = data['TotalPorchSF'].astype(int)\n\ndata.drop('TotalPorchSF_Band', axis=1, inplace=True)\ndata = pd.get_dummies(data, columns = [\"TotalPorchSF\"], prefix=\"TotalPorchSF\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "50828d0e450b267894a55994e8f6306dfe42c66d"
      },
      "cell_type": "code",
      "source": "#Pool Area\ndef PoolFlag(col):\n    if col['PoolArea'] == 0:\n        return 0\n    else:\n        return 1\ndata['HasPool_Flag'] = data.apply(PoolFlag, axis=1)\ndata.drop('PoolArea', axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "455c30b32ef95f3fd5ab74ba06678deac524a1c5"
      },
      "cell_type": "code",
      "source": "#PoolQC - Not correlated to Price will drop\ndata.drop('PoolQC', axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f2dfaf549ee0a84ed8da0da78a6c700ec30e2e9"
      },
      "cell_type": "code",
      "source": "#Fence\ndata = pd.get_dummies(data, columns = [\"Fence\"], prefix=\"Fence\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b7b54c5f230b4a54d6b199aa401c6daaffa06ef6"
      },
      "cell_type": "code",
      "source": "#Zoning Classification\ndata = pd.get_dummies(data, columns = [\"MSZoning\"], prefix=\"MSZoning\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3bd23701e997188506d9b7894debf2c6050c7d93"
      },
      "cell_type": "code",
      "source": "#Neightborhood\ndata = pd.get_dummies(data, columns = [\"Neighborhood\"], prefix=\"Neighborhood\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ebe036ee5eb47ae7f4d9eedd13041330ea833bf9"
      },
      "cell_type": "code",
      "source": "#Condition\ndata['Condition1'] = data['Condition1'].map({\"Norm\":\"Norm\", \"Feedr\":\"Street\", \"PosN\":\"Pos\", \"Artery\":\"Street\", \"RRAe\":\"Train\",\n                                                    \"RRNn\":\"Train\", \"RRAn\":\"Train\", \"PosA\":\"Pos\", \"RRNe\":\"Train\"})\ndata['Condition2'] = data['Condition2'].map({\"Norm\":\"Norm\", \"Feedr\":\"Street\", \"PosN\":\"Pos\", \"Artery\":\"Street\", \"RRAe\":\"Train\",\n                                                    \"RRNn\":\"Train\", \"RRAn\":\"Train\", \"PosA\":\"Pos\", \"RRNe\":\"Train\"})\ndef ConditionMatch(col):\n    if col['Condition1'] == col['Condition2']:\n        return 0\n    else:\n        return 1\n    \ndata['Diff2ndCondition_Flag'] = data.apply(ConditionMatch, axis=1)\ndata.drop('Condition2', axis=1, inplace=True)\n\ndata = pd.get_dummies(data, columns = [\"Condition1\"], prefix=\"Condition1\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "91571dfc971fc3fe33bc9f3869b13cc40afa8967"
      },
      "cell_type": "code",
      "source": "#Lot ARea\ndata['LotArea_Band'] = pd.qcut(data['LotArea'], 8)\ndata['LotArea_Band'].unique()\n\ndata.loc[data['LotArea']<=5684.75, 'LotArea'] = 1\ndata.loc[(data['LotArea']>5684.75) & (data['LotArea']<=7474), 'LotArea'] = 2\ndata.loc[(data['LotArea']>7474) & (data['LotArea']<=8520), 'LotArea'] = 3\ndata.loc[(data['LotArea']>8520) & (data['LotArea']<=9450), 'LotArea'] = 4\ndata.loc[(data['LotArea']>9450) & (data['LotArea']<=10355.25), 'LotArea'] = 5\ndata.loc[(data['LotArea']>10355.25) & (data['LotArea']<=11554.25), 'LotArea'] = 6\ndata.loc[(data['LotArea']>11554.25) & (data['LotArea']<=13613), 'LotArea'] = 7\ndata.loc[data['LotArea']>13613, 'LotArea'] = 8\ndata['LotArea'] = data['LotArea'].astype(int)\n\ndata.drop('LotArea_Band', axis=1, inplace=True)\ndata = pd.get_dummies(data, columns = [\"LotArea\"], prefix=\"LotArea\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6cbb06bcb50f185fc643e6eb87cd4a068b88ec94"
      },
      "cell_type": "code",
      "source": "#LotShape\ndata = pd.get_dummies(data, columns = [\"LotShape\"], prefix=\"LotShape\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4110bfac361e0d528568bd8189fb70cb2713f15"
      },
      "cell_type": "code",
      "source": "#Land Contour\ndata = pd.get_dummies(data, columns = [\"LandContour\"], prefix=\"LandContour\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5f963bb5ee8ecb5335681b8693ada3ed436e4c2"
      },
      "cell_type": "code",
      "source": "#LotConfig\ndata['LotConfig'] = data['LotConfig'].map({\"Inside\":\"Inside\", \"FR2\":\"FR\", \"Corner\":\"Corner\", \"CulDSac\":\"CulDSac\", \"FR3\":\"FR\"})\ndata = pd.get_dummies(data, columns = [\"LotConfig\"], prefix=\"LotConfig\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92b11f2d587b9e17b6e5ef9a53956fce3e838116"
      },
      "cell_type": "code",
      "source": "#Landslope\ndata['LandSlope'] = data['LandSlope'].map({\"Gtl\":1, \"Mod\":2, \"Sev\":2})\n\ndef Slope(col):\n    if col['LandSlope'] == 1:\n        return 1\n    else:\n        return 0\n    \ndata['GentleSlope_Flag'] = data.apply(Slope, axis=1)\ndata.drop('LandSlope', axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3fba6054207483b682cd9759cb505785cc3973c"
      },
      "cell_type": "code",
      "source": "#Street - No Correlation\ndata.drop('Street', axis=1, inplace=True) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e513daccf724a7c39d595813a0de50fdbe8a21bc"
      },
      "cell_type": "code",
      "source": "#Alley\ndata = pd.get_dummies(data, columns = [\"Alley\"], prefix=\"Alley\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbe780caad83feed3d4186a88914842b485959a9"
      },
      "cell_type": "code",
      "source": "#Paved Driveway\ndata = pd.get_dummies(data, columns = [\"PavedDrive\"], prefix=\"PavedDrive\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30c90996c02726a33cdc159cba4befc19842d129"
      },
      "cell_type": "code",
      "source": "#Heating\ndata['GasA_Flag'] = data['Heating'].map({\"GasA\":1, \"GasW\":0, \"Grav\":0, \"Wall\":0, \"OthW\":0, \"Floor\":0})\ndata.drop('Heating', axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a6a15f4a9bc3d5a055ebda3e4e0e3c27cfb1cdb"
      },
      "cell_type": "code",
      "source": "#Heating Quality\ndata['HeatingQC'] = data['HeatingQC'].map({\"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})\ndata['HeatingQC'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8405a91c938106f39fc01bbaaa5d657f79a7f68e"
      },
      "cell_type": "code",
      "source": "#Central Air\ndata['CentralAir'] = data['CentralAir'].map({\"Y\":1, \"N\":0})\ndata['CentralAir'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44bb9e24e3dc1c53b55fa9e2c0775fde4b48d43c"
      },
      "cell_type": "code",
      "source": "#Electrical\ndata['Electrical'] = data['Electrical'].map({\"SBrkr\":\"SBrkr\", \"FuseF\":\"Fuse\", \"FuseA\":\"Fuse\", \"FuseP\":\"Fuse\", \"Mix\":\"Mix\"})\ndata = pd.get_dummies(data, columns = [\"Electrical\"], prefix=\"Electrical\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f7bc0bf87e90e04d286045cd735804d8c8779b9"
      },
      "cell_type": "code",
      "source": "#MiscFEature - drop no correlation\ncolumns=['MiscFeature', 'MiscVal']\ndata.drop(columns, axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d9848ba78473458621b399a3e004ec9f868a3c9"
      },
      "cell_type": "code",
      "source": "#MoSold\ndata = pd.get_dummies(data, columns = [\"MoSold\"], prefix=\"MoSold\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dee43c8ca34c3afc1af737e4aa4169372860cfe8"
      },
      "cell_type": "code",
      "source": "#YearSold\ndata = pd.get_dummies(data, columns = [\"YrSold\"], prefix=\"YrSold\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eccf86f72b4efe0cc0498f616751c0e92cab6502"
      },
      "cell_type": "code",
      "source": "#Saletype\ndata['SaleType'] = data['SaleType'].map({\"WD\":\"WD\", \"New\":\"New\", \"COD\":\"COD\", \"CWD\":\"CWD\", \"ConLD\":\"Oth\", \"ConLI\":\"Oth\", \n                                                 \"ConLw\":\"Oth\", \"Con\":\"Oth\", \"Oth\":\"Oth\"})\ndata = pd.get_dummies(data, columns = [\"SaleType\"], prefix=\"SaleType\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7435c49b06b4fe25d85bdb468822c59e55cbe3fa"
      },
      "cell_type": "code",
      "source": "#Sale Condition\ndata = pd.get_dummies(data, columns = [\"SaleCondition\"], prefix=\"SaleCondition\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "17c282bc19bfd66f9dd0cd316da65682cace01d3"
      },
      "cell_type": "markdown",
      "source": "Now that the data is now all the same, we are going to be **Looking at the Distribution**\n\nFrom what we could tell is that it is positeveley skewed and we have to make a normal distribution"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "148334b796257b550ee1c0a6a0bdc0aebdb98ffa"
      },
      "cell_type": "code",
      "source": "#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ntargetval = train[\"SalePrice\"]\n\n#Check the new distribution \nplt.subplots(figsize=(15, 10))\ng = sns.distplot(train['SalePrice'], fit=norm, label = \"Skewness : %.2f\"%(train['SalePrice'].skew()));\ng = g.legend(loc=\"best\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7063cc499a2f655aeffe3efa09ab739cf8eb1ba8"
      },
      "cell_type": "code",
      "source": "# First lets single out the numeric features\nnumeric_feats = data.dtypes[data.dtypes != \"object\"].index\n\n# Check how skewed they are\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n\nplt.subplots(figsize =(65, 20))\nskewed_feats.plot(kind='bar');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fb74cb22717f763ea4a0d73919c04a952ac6a076"
      },
      "cell_type": "markdown",
      "source": "**Box Cox Transformation**\n\nIt is:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c827a4a15e5445d27cc881c06680612435c9ed0"
      },
      "cell_type": "code",
      "source": "skewness = skewed_feats[abs(skewed_feats) > 0.5]\n\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    data[feat] = boxcox1p(data[feat], lam)\n\nprint(skewness.shape[0],  \"skewed numerical features have been Box-Cox transformed\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4df4ab2c6797967d9325118a1ab4911f169a8594"
      },
      "cell_type": "markdown",
      "source": "**Data Modelling**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f410221744b44df3d2abed20c56afb8ae4a825c"
      },
      "cell_type": "code",
      "source": "# First, re-create the training and test datasets\ntrain = data[:ntrain]\ntest = data[ntrain:]\n\nprint(train.shape)\nprint(test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "323607a92dee1088120ae68e6012e5a349deef85"
      },
      "cell_type": "code",
      "source": "import xgboost as xgb\n\nmodel = xgb.XGBRegressor()\nmodel.fit(train, targetval)\n\n# Sort feature importances from GBC model trained earlier\nindices = np.argsort(model.feature_importances_)[::-1]\nindices = indices[:75]\n\n# Visualise these with a barplot\nplt.subplots(figsize=(20, 15))\ng = sns.barplot(y=train.columns[indices], x = model.feature_importances_[indices], orient='h', palette = mycols)\ng.set_xlabel(\"Relative importance\",fontsize=12)\ng.set_ylabel(\"Features\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title(\"XGB feature importance\");",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86a982675b20f6e224327013483fcc6bf09fba8d"
      },
      "cell_type": "code",
      "source": "xgb_train = train.copy()\nxgb_test = test.copy()\n\nimport xgboost as xgb\nmodel = xgb.XGBRegressor()\nmodel.fit(xgb_train, targetval)\n\n# Allow the feature importances attribute to select the most important features\nxgb_feat_red = SelectFromModel(model, prefit = True)\n\n# Reduce estimation, validation and test datasets\nxgb_train = xgb_feat_red.transform(xgb_train)\nxgb_test = xgb_feat_red.transform(xgb_test)\n\n\nprint(\"Results of 'feature_importances_':\")\nprint('X_train: ', xgb_train.shape, '\\nX_test: ', xgb_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f56c962552db2dccb5f231a12077b60a47d40a93"
      },
      "cell_type": "code",
      "source": "# Next we want to sample our training data to test for performance of robustness ans accuracy, before applying to the test data\nX_train, X_test, targetval, Y_test = model_selection.train_test_split(xgb_train, targetval, test_size=0.3, random_state=42)\n\nprint('X_train: ', X_train.shape, '\\nX_test: ', X_test.shape, '\\ntargetval: ', targetval.shape, '\\nY_test: ', Y_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "98ec0ec3856383886dbb5c2cdb3d8a56527029a7"
      },
      "cell_type": "markdown",
      "source": "Prediction"
    },
    {
      "metadata": {
        "_uuid": "2da79eb63c945a220f5842bafcf052fda0cccd16",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import xgboost as xgb\n#Machine Learning Algorithm (MLA) Selection and Initialization\nmodels = [KernelRidge(), ElasticNet(), Lasso(), GradientBoostingRegressor(), BayesianRidge(), LassoLarsIC(), RandomForestRegressor(), xgb.XGBRegressor()]\n\n# First I will use ShuffleSplit as a way of randomising the cross validation samples.\nshuff = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n\n#create table to compare MLA metrics\ncolumns = ['Name', 'Parameters', 'Train Accuracy Mean', 'Test Accuracy']\nbefore_model_compare = pd.DataFrame(columns = columns)\n\n#index through models and save performance to table\nrow_index = 0\nfor alg in models:\n\n    #set name and parameters\n    model_name = alg.__class__.__name__\n    before_model_compare.loc[row_index, 'Name'] = model_name\n    before_model_compare.loc[row_index, 'Parameters'] = str(alg.get_params())\n    \n    alg.fit(X_train, targetval)\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    training_results = np.sqrt((-cross_val_score(alg, X_train, targetval, cv = shuff, scoring= 'neg_mean_squared_error')).mean())\n    test_results = np.sqrt(((Y_test-alg.predict(X_test))**2).mean())\n    \n    before_model_compare.loc[row_index, 'Train Accuracy Mean'] = (training_results)*100\n    before_model_compare.loc[row_index, 'Test Accuracy'] = (test_results)*100\n    \n    row_index+=1\n    print(row_index, alg.__class__.__name__, 'trained...')\n\ndecimals = 3\nbefore_model_compare['Train Accuracy Mean'] = before_model_compare['Train Accuracy Mean'].apply(lambda x: round(x, decimals))\nbefore_model_compare['Test Accuracy'] = before_model_compare['Test Accuracy'].apply(lambda x: round(x, decimals))\nbefore_model_compare",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c735bf9fa04f256ab8be7b8e8c9c7c6a8728e35d"
      },
      "cell_type": "code",
      "source": "models = [KernelRidge(), ElasticNet(), Lasso(), GradientBoostingRegressor(), BayesianRidge(), LassoLarsIC(), RandomForestRegressor(), xgb.XGBRegressor()]\n\nKR_param_grid = {'alpha': [0.1], 'coef0': [100], 'degree': [1], 'gamma': [None], 'kernel': ['polynomial']}\nEN_param_grid = {'alpha': [0.001], 'copy_X': [True], 'l1_ratio': [0.6], 'fit_intercept': [True], 'normalize': [False], \n                         'precompute': [False], 'max_iter': [300], 'tol': [0.001], 'selection': ['random'], 'random_state': [None]}\nLASS_param_grid = {'alpha': [0.0005], 'copy_X': [True], 'fit_intercept': [True], 'normalize': [False], 'precompute': [False], \n                    'max_iter': [300], 'tol': [0.01], 'selection': ['random'], 'random_state': [None]}\nGB_param_grid = {'loss': ['huber'], 'learning_rate': [0.1], 'n_estimators': [300], 'max_depth': [3], \n                                        'min_samples_split': [0.0025], 'min_samples_leaf': [5]}\nBR_param_grid = {'n_iter': [200], 'tol': [0.00001], 'alpha_1': [0.00000001], 'alpha_2': [0.000005], 'lambda_1': [0.000005], \n                 'lambda_2': [0.00000001], 'copy_X': [True]}\nLL_param_grid = {'criterion': ['aic'], 'normalize': [True], 'max_iter': [100], 'copy_X': [True], 'precompute': ['auto'], 'eps': [0.000001]}\nRFR_param_grid = {'n_estimators': [50], 'max_features': ['auto'], 'max_depth': [None], 'min_samples_split': [5], 'min_samples_leaf': [2]}\nXGB_param_grid = {'max_depth': [3], 'learning_rate': [0.1], 'n_estimators': [300], 'booster': ['gbtree'], 'gamma': [0], 'reg_alpha': [0.1],\n                  'reg_lambda': [0.7], 'max_delta_step': [0], 'min_child_weight': [1], 'colsample_bytree': [0.5], 'colsample_bylevel': [0.2],\n                  'scale_pos_weight': [1]}\nparams_grid = [KR_param_grid, EN_param_grid, LASS_param_grid, GB_param_grid, BR_param_grid, LL_param_grid, RFR_param_grid, XGB_param_grid]\n\nafter_model_compare = pd.DataFrame(columns = columns)\n\nrow_index = 0\nfor alg in models:\n    \n    gs_alg = GridSearchCV(alg, param_grid = params_grid[0], cv = shuff, scoring = 'neg_mean_squared_error', n_jobs=-1)\n    params_grid.pop(0)\n\n    #set name and parameters\n    model_name = alg.__class__.__name__\n    after_model_compare.loc[row_index, 'Name'] = model_name\n    \n    gs_alg.fit(X_train, targetval)\n    gs_best = gs_alg.best_estimator_\n    after_model_compare.loc[row_index, 'Parameters'] = str(gs_alg.best_params_)\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    after_training_results = np.sqrt(-gs_alg.best_score_)\n    after_test_results = np.sqrt(((Y_test-gs_alg.predict(X_test))**2).mean())\n    \n    after_model_compare.loc[row_index, 'Train Accuracy Mean'] = (after_training_results)*100\n    after_model_compare.loc[row_index, 'Test Accuracy'] = (after_test_results)*100\n    \n    row_index+=1\n    print(row_index, alg.__class__.__name__, 'trained...')\n\ndecimals = 3\nafter_model_compare['Train Accuracy Mean'] = after_model_compare['Train Accuracy Mean'].apply(lambda x: round(x, decimals))\nafter_model_compare['Test Accuracy'] = after_model_compare['Test Accuracy'].apply(lambda x: round(x, decimals))\nafter_model_compare",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ac7a715000ea7f7e43db047ed9b274597368c0b"
      },
      "cell_type": "code",
      "source": "models = [KernelRidge(), ElasticNet(), Lasso(), GradientBoostingRegressor(), BayesianRidge(), LassoLarsIC(), RandomForestRegressor(), xgb.XGBRegressor()]\nnames = ['KernelRidge', 'ElasticNet', 'Lasso', 'Gradient Boosting', 'Bayesian Ridge', 'Lasso Lars IC', 'Random Forest', 'XGBoost']\nparams_grid = [KR_param_grid, EN_param_grid, LASS_param_grid, GB_param_grid, BR_param_grid, LL_param_grid, RFR_param_grid, XGB_param_grid]\nstacked_validation_train = pd.DataFrame()\nstacked_test_train = pd.DataFrame()\n\nrow_index=0\n\nfor alg in models:\n    \n    gs_alg = GridSearchCV(alg, param_grid = params_grid[0], cv = shuff, scoring = 'neg_mean_squared_error', n_jobs=-1)\n    params_grid.pop(0)\n    \n    gs_alg.fit(X_train, targetval)\n    gs_best = gs_alg.best_estimator_\n    stacked_validation_train.insert(loc = row_index, column = names[0], value = gs_best.predict(X_test))\n    print(row_index+1, alg.__class__.__name__, 'predictions added to stacking validation dataset...')\n    \n    stacked_test_train.insert(loc = row_index, column = names[0], value = gs_best.predict(xgb_test))\n    print(row_index+1, alg.__class__.__name__, 'predictions added to stacking test dataset...')\n    print(\"-\"*50)\n    names.pop(0)\n    \n    row_index+=1\n    \nprint('Done')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b2e92edb4e139f6484f142d5a9b862d871caf6c"
      },
      "cell_type": "code",
      "source": "stacked_validation_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f613d0eb07e755e398dd9f50a8e5580de92a8a0"
      },
      "cell_type": "code",
      "source": "stacked_test_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbf50afc2a878017fa9fddfb185130ebedc88e2d"
      },
      "cell_type": "code",
      "source": "# First drop the Lasso results from the table, as we will be using Lasso as the meta-model\ndrop = ['Lasso']\nstacked_validation_train.drop(drop, axis=1, inplace=True)\nstacked_test_train.drop(drop, axis=1, inplace=True)\n\n# Now fit the meta model and generate predictions\nmeta_model = make_pipeline(RobustScaler(), Lasso(alpha=0.00001, copy_X = True, fit_intercept = True,\n                                              normalize = False, precompute = False, max_iter = 10000,\n                                              tol = 0.0001, selection = 'random', random_state = None))\nmeta_model.fit(stacked_validation_train, Y_test)\n\nmeta_model_pred = np.expm1(meta_model.predict(stacked_test_train))\nprint(\"Meta-model trained and applied!...\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec74d2a698f6d06afa8bf437a028accdf224f07e"
      },
      "cell_type": "code",
      "source": "models = [KernelRidge(), ElasticNet(), Lasso(), GradientBoostingRegressor(), BayesianRidge(), LassoLarsIC(), RandomForestRegressor(), xgb.XGBRegressor()]\nnames = ['KernelRidge', 'ElasticNet', 'Lasso', 'Gradient Boosting', 'Bayesian Ridge', 'Lasso Lars IC', 'Random Forest', 'XGBoost']\nparams_grid = [KR_param_grid, EN_param_grid, LASS_param_grid, GB_param_grid, BR_param_grid, LL_param_grid, RFR_param_grid, XGB_param_grid]\nfinal_predictions = pd.DataFrame()\n\nrow_index=0\n\nfor alg in models:\n    \n    gs_alg = GridSearchCV(alg, param_grid = params_grid[0], cv = shuff, scoring = 'neg_mean_squared_error', n_jobs=-1)\n    params_grid.pop(0)\n    \n    gs_alg.fit(stacked_validation_train, Y_test)\n    gs_best = gs_alg.best_estimator_\n    final_predictions.insert(loc = row_index, column = names[0], value = np.expm1(gs_best.predict(stacked_test_train)))\n    print(row_index+1, alg.__class__.__name__, 'final results predicted added to table...')\n    names.pop(0)\n    \n    row_index+=1\n\nprint(\"-\"*50)\nprint(\"Done\")\n    \nfinal_predictions.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b628e9b6394570ba6e0425fdc36c8667ab41013a"
      },
      "cell_type": "code",
      "source": "ensemble = meta_model_pred*(1/10) + final_predictions['XGBoost']*(1.5/10) + final_predictions['Gradient Boosting']*(2/10) + final_predictions['Bayesian Ridge']*(1/10) + final_predictions['Lasso']*(1/10) + final_predictions['KernelRidge']*(1/10) + final_predictions['Lasso Lars IC']*(1/10) + final_predictions['Random Forest']*(1.5/10)\n\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = ensemble\n#submission.to_csv('final_submission.csv',index=False)\nprint(\"Submission file, created!\")",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}